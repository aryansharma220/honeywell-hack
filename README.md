# 🔍 Time Series Anomaly Detection System

A comprehensive Python system for detecting anomalies in multivariate time series data with **multiple interactive user interfaces** and advanced visualization capabilities.

## 🌟 New Features & Enhanced UI

### 🎛️ Multiple User Interfaces
- **🚀 Interactive Launcher**: Easy-to-use menu system for all features
- **🌐 Streamlit Dashboard**: Modern web-based interface with real-time interaction
- **📊 Real-time Monitor**: Live monitoring dashboard with alerts and notifications
- **📈 Enhanced Demo**: Rich visualizations with animated output and charts
- **🔧 Command Line**: Traditional CLI with improved output formatting

### 🎨 Advanced Visualizations
- **Interactive Time Series Plots**: Plotly-powered charts with zoom, pan, hover
- **Feature Importance Analysis**: Bar charts, pie charts, and heatmaps
- **Real-time Monitoring**: Live updating charts and alert systems  
- **Statistical Dashboards**: Comprehensive analytics with multiple chart types
- **Export Capabilities**: Save charts as PNG, HTML, and interactive formats

### 📊 Dashboard Features
- **Drag-and-drop file upload**
- **Configurable parameters** (contamination rate, thresholds)
- **Interactive filtering and selection**
- **Downloadable results** in multiple formats
- **Real-time metrics** and alerts
- **Responsive design** for desktop and mobile

## 🚀 Quick Start

### Option 1: Interactive Launcher (Recommended)
```bash
python launcher.py
```
This provides a menu-driven interface to access all features!

### Option 2: Streamlit Web Dashboard
```bash
streamlit run dashboard.py
```
Then open http://localhost:8501 in your browser

### Option 3: Real-time Monitoring
```bash
python realtime_monitor.py
```
Then open http://localhost:8050 in your browser

### Option 4: Enhanced Demo
```bash
python demo.py
```

## 📋 Installation

### Quick Install
```bash
pip install -r requirements.txt
```

### Dependencies
- Core: `pandas`, `numpy`, `scikit-learn`
- Visualization: `matplotlib`, `seaborn`, `plotly`
- **0-100 Scoring**: Easy to interpret anomaly scores
- **Feature Threshold**: Minimum contribution for top features (0.1-10.0)
- **Principle**: Anomalies are easier to isolate than normal points

[Project] Time Series Anomaly Detection

This repository contains a self-contained Python project that detects anomalies in multivariate time series data and reports the top contributing features for each anomalous row.

Core capabilities
- Isolation Forest based unsupervised anomaly detection
- Per-row feature contribution scoring (top N contributors)
- Multiple presentation layers: CLI launcher, script demo, Streamlit dashboard, and a Dash-based real-time monitor (simulation)
- Exports: CSV results and visualizations generated by the demo

Files included
- `anomaly_detector.py` — core detection logic and feature attribution (IsolationForest + heuristics)
- `utils.py` — parsing, validation, helpers, and export helpers
- `demo.py` — demonstration script that generates plots and saves images/CSV
- `dashboard.py` — Streamlit dashboard UI (file upload + interactive charts)
- `realtime_monitor.py` — Dash app that simulates a live monitoring dashboard
- `launcher.py` — simple menu-driven launcher to run demo, dashboard, or CLI analysis
- `config.py` — central configuration constants used across the project
- `requirements.txt` — Python dependencies list
- `sample_dataset.csv` — sample multivariate time series used for demos/tests

Quick start (recommended for judges)

1) Create and activate a virtual environment (PowerShell example):

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

2) Run the interactive launcher (recommended):

```powershell
python launcher.py
```

The launcher offers options to run the demo visualizations, open the Streamlit dashboard, or run command-line analysis.

Alternative quick commands
- Run the demo script (generates `anomaly_results_demo.csv` and plots):

```powershell
python demo.py
```

- Launch the Streamlit dashboard (open the shown URL in a browser):

```powershell
streamlit run dashboard.py
```

- Launch the Dash-based real-time monitor (runs a local server):

```powershell
python realtime_monitor.py
```

Behavior & expected outputs
- The detection adds the following columns to the original dataset and saves results when run via the provided scripts:
	- `Abnormality_score` — calibrated score in range 0–100
	- `top_feature_1` .. `top_feature_7` — feature names that contributed most to the anomaly (empty string when fewer contributors)
- Demo run will save a CSV such as `anomaly_results_demo.csv` and image files like `anomaly_timeseries.png`, `feature_importance.png`, and `advanced_analytics.png` if the plotting libraries are available.

Notes & limitations (important for judges)
- Single-command wrapper referenced in older docs (`run_demo.py`) is not present — use `python demo.py` or the `launcher.py` menu instead.
- The Streamlit and Dash UIs require the corresponding packages to be installed (`streamlit`, `dash`). If those packages are not available, run `demo.py` and inspect the generated CSV and PNGs as fallback.
- The project expects a `Time` column in the CSV and uses the format `%m/%d/%Y %H:%M` by default (see `config.DATETIME_FORMAT`). `utils.parse_datetime` accepts several common formats, but mismatched time formats can cause parsing errors.

Recommended judge runbook (1–2 minutes)
1) Install requirements in a venv (see Quick start).
2) Run `python launcher.py`, choose option 1 (Demo) or directly run `python demo.py`.
3) Inspect `anomaly_results_demo.csv` for the `Abnormality_score` column and `top_feature_1..7`.
4) If you want an interactive exploration, run `streamlit run dashboard.py` and upload `anomaly_results_demo.csv` or the original `sample_dataset.csv`.

How the detection works (brief)
- Training period is read from `config.py` (TRAINING_START / TRAINING_END). The model trains an Isolation Forest on the normal period and scores the analysis period.
- Raw model outputs are calibrated to 0–100 scores and per-row feature importance is estimated by normalized, weighted deviations from training statistics. The top contributors are reported per row.

If you need help or want a one-minute recorded walkthrough, see the repository owner notes.

License
- MIT

Acknowledgements
- Built quickly for a hackathon-style project; focuses on end-to-end detection, attribution, and presentation.
- **Excel**: Multi-sheet workbooks with summaries
