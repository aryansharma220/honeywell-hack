# Time Series Anomaly Detection System

A comprehensive Python system for detecting anomalies in multivariate time series data with multiple interactive user interfaces and advanced visualization capabilities.

## New Features & Enhanced UI

### Multiple User Interfaces
- Interactive Launcher: Easy-to-use menu system for all features
- Streamlit Dashboard: Modern web-based interface with real-time interaction
- Real-time Monitor: Live monitoring dashboard with alerts and notifications
- Enhanced Demo: Rich visualizations with animated output and charts
- Command Line: Traditional CLI with improved output formatting

### Advanced Visualizations
- Interactive Time Series Plots: Plotly-powered charts with zoom, pan, hover
- Feature Importance Analysis: Bar charts, pie charts, and heatmaps
- Real-time Monitoring: Live updating charts and alert systems
- Statistical Dashboards: Comprehensive analytics with multiple chart types
- Export Capabilities: Save charts as PNG, HTML, and interactive formats

### Dashboard Features
- Drag-and-drop file upload
- Configurable parameters (contamination rate, thresholds)
- Interactive filtering and selection
- Downloadable results in multiple formats
- Real-time metrics and alerts
- Responsive design for desktop and mobile

## Quick Start

### Option 1: Interactive Launcher (Recommended)
```bash
python launcher.py
```
This provides a menu-driven interface to access all features.

### Option 2: Streamlit Web Dashboard
```bash
streamlit run dashboard.py
```
Then open http://localhost:8501 in your browser.

### Option 3: Real-time Monitoring
```bash
python realtime_monitor.py
```
Then open http://localhost:8050 in your browser.

### Option 4: Enhanced Demo
```bash
python demo.py
```

## Installation

### Quick Install
```bash
pip install -r requirements.txt
```

### Dependencies
- Core: `pandas`, `numpy`, `scikit-learn`
- Visualization: `matplotlib`, `seaborn`, `plotly`


Project overview

This repository contains a self-contained Python project that detects anomalies in multivariate time series data and reports the top contributing features for each anomalous row.

Core capabilities
- Isolation Forest based unsupervised anomaly detection
- Per-row feature contribution scoring (top N contributors)
- Multiple presentation layers: CLI launcher, demo script, Streamlit dashboard, and a Dash-based real-time monitor (simulation)
- Exports: CSV results and visualizations generated by the demo

Files included
- `anomaly_detector.py` — core detection logic and feature attribution (IsolationForest + heuristics)
- `utils.py` — parsing, validation, helpers, and export helpers
- `demo.py` — demonstration script that generates plots and saves images/CSV
- `dashboard.py` — Streamlit dashboard UI (file upload + interactive charts)
- `realtime_monitor.py` — Dash app that simulates a live monitoring dashboard
- `launcher.py` — simple menu-driven launcher to run demo, dashboard, or CLI analysis
- `config.py` — central configuration constants used across the project
- `requirements.txt` — Python dependencies list
- `sample_dataset.csv` — sample multivariate time series used for demos/tests

Quick start 
1) Create and activate a virtual environment (PowerShell example):

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

2) Run the interactive launcher (recommended):

```powershell
python launcher.py
```

The launcher offers options to run the demo visualizations, open the Streamlit dashboard, or run command-line analysis.

Alternative quick commands
- Run the demo script (generates `anomaly_results_demo.csv` and plots):

```powershell
python demo.py
```

- Launch the Streamlit dashboard (open the shown URL in a browser):

```powershell
streamlit run dashboard.py
```

- Launch the Dash-based real-time monitor (runs a local server):

```powershell
python realtime_monitor.py
```

Behavior & expected outputs
- The detection adds the following columns to the original dataset and saves results when run via the provided scripts:
	- `Abnormality_score` — calibrated score in range 0–100
	- `top_feature_1` .. `top_feature_7` — feature names that contributed most to the anomaly (empty string when fewer contributors)
- Demo run will save a CSV such as `anomaly_results_demo.csv` and image files like `anomaly_timeseries.png`, `feature_importance.png`, and `advanced_analytics.png` if the plotting libraries are available.

Notes & limitations
- Single-command wrapper referenced in older docs (`run_demo.py`) is not present — use `python demo.py` or the `launcher.py` menu instead.
- The Streamlit and Dash UIs require the corresponding packages to be installed (`streamlit`, `dash`). If those packages are not available, run `demo.py` and inspect the generated CSV and PNGs as fallback.
- The project expects a `Time` column in the CSV and uses the format `%m/%d/%Y %H:%M` by default (see `config.DATETIME_FORMAT`). `utils.parse_datetime` accepts several common formats, but mismatched time formats can cause parsing errors.

Recommended judge runbook (1–2 minutes)
1) Install requirements in a venv (see Quick start).
2) Run `python launcher.py`, choose option 1 (Demo) or directly run `python demo.py`.
3) Inspect `anomaly_results_demo.csv` for the `Abnormality_score` column and `top_feature_1..7`.
4) If you want an interactive exploration, run `streamlit run dashboard.py` and upload `anomaly_results_demo.csv` or the original `sample_dataset.csv`.

How the detection works (brief)
- Training period is read from `config.py` (TRAINING_START / TRAINING_END). The model trains an Isolation Forest on the normal period and scores the analysis period.
- Raw model outputs are calibrated to 0–100 scores and per-row feature importance is estimated by normalized, weighted deviations from training statistics. The top contributors are reported per row.


